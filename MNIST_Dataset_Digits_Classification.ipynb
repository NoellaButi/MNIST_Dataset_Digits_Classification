{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bbcc86e-cadf-42c7-9e1e-138ea8377252",
   "metadata": {},
   "source": [
    "# MNIST Dataset Digits Classification\n",
    "\n",
    "## Objective:\n",
    "Build a multi-layer neural network to classify handwritten digits (MNIST-like dataset).\n",
    "\n",
    "## Steps Taken:\n",
    "- **Libraries:** Used `numpy` for matrix operations and `pandas` for data loading, along with `OneHotEncoder` for label encoding.\n",
    "- **Data Preprocessing:** Normalized pixel values and one-hot encoded the labels.\n",
    "- **Model Design:**\n",
    "  - **Forward Pass:** Used the sigmoid function for hidden layers and softmax for output layers.\n",
    "  - **Loss Function:** Employed cross-entropy loss for optimization.\n",
    "  - **Training:** Updated weights and biases using backpropagation and gradient descent.\n",
    "- **Evaluation:** Tested the model on a test dataset and calculated accuracy.\n",
    "\n",
    "## Learning Style:\n",
    "- Gradient-based optimization and backpropagation.\n",
    "- Continuous features like pixel intensities (numerical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944e9f6-5dd7-4343-afae-c0b868d42289",
   "metadata": {},
   "source": [
    "# Step 1: Importing Libraries\n",
    "\n",
    "We began by importing the necessary libraries. For this task, we used `numpy` for numerical computations and `pandas` for loading the CSV data files. These libraries are essential for handling the dataset and performing matrix operations efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f82cd69-d1ee-4c81-bb90-ffd60e2b66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a93557-c199-4227-9a05-82c12523a97a",
   "metadata": {},
   "source": [
    "# Step 2: Loading the Data\n",
    "\n",
    "Training data, training labels, testing data, and testing labels. We will load these datasets using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a92fcda-e0a9-4388-b25f-17eb4a9cd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and labels\n",
    "X_train = pd.read_csv('data/training60000.csv', header=None).values\n",
    "y_train = pd.read_csv('data/training60000_labels.csv', header=None).values\n",
    "\n",
    "# Load testing data and labels\n",
    "X_test = pd.read_csv('data/testing10000.csv', header=None).values\n",
    "y_test = pd.read_csv('data/testing10000_labels.csv', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c32869-4cab-40f4-9a0d-448bed9f5204",
   "metadata": {},
   "source": [
    "# Step 3: Preprocessing the Data\n",
    "\n",
    "Normalize the input data (range normalization) and convert labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cc1df7a-c935-48f9-a923-a3ab86a1e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data\n",
    "X_train = (X_train - 0.01) / (1 - 0.01)  # Normalize using the given formula\n",
    "X_test = (X_test - 0.01) / (1 - 0.01)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_one_hot = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690558e2-0db6-4f19-8294-a1a315f38382",
   "metadata": {},
   "source": [
    "# Step 4: Defining Activation Functions\n",
    "\n",
    "We'll need the logistic sigmoid function for the hidden layer and the softmax function for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7845fbf-fb82-4b28-a342-5a8a2499642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic (Sigmoid) function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Derivative of the logistic function (for backpropagation)\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability improvement\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function (for output layer)\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / y_true.shape[0]  # Add small value to prevent log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ca71c-8886-4673-ad95-85e1a4121e23",
   "metadata": {},
   "source": [
    "# Step 5: Initializing Network Parameters\n",
    "\n",
    "Now, we'll initialize the weights and biases for each layer. For simplicity, let's use random initialization for the weights and zeros for the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2e73e63-cfab-41b7-8ec9-f46217cd57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of units in each layer\n",
    "input_size = 784  # 28x28 pixels\n",
    "hidden_size = 128  # You can experiment with different sizes here\n",
    "output_size = 10  # Digits 0-9\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c7668-3e20-4fea-9296-e16574ad6776",
   "metadata": {},
   "source": [
    "# Step 6: Forward Pass\n",
    "\n",
    "Now, we will implement the forward pass, where we compute the activations for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73c7984a-afb1-4293-85d7-e3b42874f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def forward(X):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)  # Hidden layer activation\n",
    "    \n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = softmax(z2)  # Output layer activation\n",
    "    \n",
    "    return a1, a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fbf97-08d1-4e6b-ae5a-a67b0ee85a80",
   "metadata": {},
   "source": [
    "# Step 7: Backpropagation\n",
    "\n",
    "The backpropagation algorithm will compute the gradients of the loss function with respect to the weights and biases and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56b1ad62-60bc-4bc7-a553-46bcb69ff2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "def backprop(X, y, a1, a2):\n",
    "    # Output layer gradients (softmax + cross-entropy)\n",
    "    delta2 = a2 - y  # Derivative of loss with respect to a2\n",
    "    \n",
    "    # Hidden layer gradients\n",
    "    delta1 = np.dot(delta2, W2.T) * sigmoid_derivative(a1)  # Backprop through hidden layer\n",
    "    \n",
    "    # Gradients for weights and biases\n",
    "    dW2 = np.dot(a1.T, delta2) / X.shape[0]\n",
    "    db2 = np.sum(delta2, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    dW1 = np.dot(X.T, delta1) / X.shape[0]\n",
    "    db1 = np.sum(delta1, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26814c-7cf0-424a-982d-471fed67f97a",
   "metadata": {},
   "source": [
    "# Step 8: Training the Network\n",
    "\n",
    "We will now train the network by iterating over the training data and updating the weights using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dada740f-56c5-43b7-9f71-f9ddc94c058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(X_train, y_train, epochs=10, learning_rate=0.1, batch_size=64):\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(num_batches):\n",
    "            # Get the batch data\n",
    "            batch_X = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            a1, a2 = forward(batch_X)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dW1, db1, dW2, db2 = backprop(batch_X, batch_y, a1, a2)\n",
    "            \n",
    "            # Update weights and biases using gradient descent\n",
    "            W1 -= learning_rate * dW1\n",
    "            b1 -= learning_rate * db1\n",
    "            W2 -= learning_rate * dW2\n",
    "            b2 -= learning_rate * db2\n",
    "        \n",
    "        # Print loss after each epoch\n",
    "        _, a2 = forward(X_train)\n",
    "        loss = cross_entropy_loss(a2, y_train)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03708a-ffe3-4cb2-bd26-97f8977a1765",
   "metadata": {},
   "source": [
    "# Step 9: Evaluating the Model\n",
    "\n",
    "Finally, we will test the model and calculate the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31e29db9-28ec-4cf2-a742-1f1f80069f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_test, y_test):\n",
    "    # Perform forward pass\n",
    "    _, a2 = forward(X_test)\n",
    "    predictions = np.argmax(a2, axis=1)\n",
    "    correct = np.sum(predictions == np.argmax(y_test, axis=1))\n",
    "    incorrect = y_test.shape[0] - correct\n",
    "    accuracy = correct / y_test.shape[0] * 100\n",
    "\n",
    "    # Print results\n",
    "    print(\"==== Results\")\n",
    "    print(f\"Network properties:  Input: {input_size}, Hidden: {hidden_size}, Output: {output_size}\")\n",
    "    print(f\"Correct classifications: {correct}\")\n",
    "    print(f\"Incorrect classifications: {incorrect}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2785b-dfb4-4f99-a8ec-7a1577d4678e",
   "metadata": {},
   "source": [
    "# Step 10: Putting It All Together\n",
    "\n",
    "Now we can train the network and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52abc51a-ac17-4adf-956a-593e51a31bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5075\n",
      "Epoch 2/10, Loss: 0.3613\n",
      "Epoch 3/10, Loss: 0.3202\n",
      "Epoch 4/10, Loss: 0.3007\n",
      "Epoch 5/10, Loss: 0.2892\n",
      "Epoch 6/10, Loss: 0.2814\n",
      "Epoch 7/10, Loss: 0.2757\n",
      "Epoch 8/10, Loss: 0.2711\n",
      "Epoch 9/10, Loss: 0.2674\n",
      "Epoch 10/10, Loss: 0.2643\n",
      "==== Results\n",
      "Network properties:  Input: 784, Hidden: 128, Output: 10\n",
      "Correct classifications: 9170\n",
      "Incorrect classifications: 830\n",
      "Accuracy: 91.70%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "train(X_train, y_train_one_hot, epochs=10, learning_rate=0.1, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(X_test, y_test_one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
